# -*- coding: utf-8 -*-
"""資料作成#5-5-PyTorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gHGbGGsshqCj8BdQH1KF083e9e0BY7b6
"""

import time
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 乱数とGPU設定
torch.manual_seed(19)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#device = "cpu"
#print(device)

# データ読み込み
digits = load_digits()
X_orig, Y_orig = digits.data, digits.target
X_train, X_test, Y_train, Y_test = \
    train_test_split(X_orig, Y_orig, test_size=0.2, random_state=19)

# Tensor化（＝PyTorch用に変換）
X_train = torch.tensor(X_train, dtype=torch.float32)
Y_train = torch.tensor(Y_train, dtype=torch.long)
X_test = torch.tensor(X_test, dtype=torch.float32).to(device)
Y_test = torch.tensor(Y_test, dtype=torch.long).to(device)

# DataLoader準備（＝ミニバッチ学習用に整形）
train_loader = DataLoader(TensorDataset(X_train, Y_train), \
                batch_size=50, shuffle=True)

# ニューラルネットワーク定義
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(64, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, 128)
        self.fc4 = nn.Linear(128, 10)
        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, \
                        nonlinearity='relu')  # He初期化

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        return self.fc4(x) # CrossEntropyLossがsoftmax込みなので省略

""" 状況表示 """
def graph(loss, acc, times, y_range=[0,2], y_step=0.2, title="loss, accuracy"):
    plt.rcParams["font.size"] = 14
    fig, ax1 = plt.subplots(figsize=(8.0, 6.0))
    step = len(loss)
    idx_mid, idx_max = int(step/2), step - 1
    t = np.linspace(0, idx_max, step)  # start, end, step

    # loss軸設定
    ax1.set_title(title, fontsize=13)
    ax1.set_xlabel('epoch')
    ax1.set_xlim([-5, idx_max+5]) # -1
    ax1.tick_params(axis='y', labelcolor='blue')
    ax1.grid(True, axis='x')

    # loss plot
    ax1.plot(t, loss, color="blue", label="Loss")
    ax1.plot(0, loss[0], color="blue",marker = 'x', markersize = 15)
    ax1.plot(idx_mid, loss[idx_mid], color="blue",marker = 'x', markersize = 15)
    ax1.plot(idx_max, loss[idx_max], color="blue",marker = 'x', markersize = 15)

    # accuracy軸設定
    ax2 = ax1.twinx()
    ax2.set_ylabel('accuracy', color='red')
    ax2.set_ylim([0, 1])
    ax2.set_yticks(np.arange(0, 1, 0.1))
    ax2.tick_params(axis='y', right=True, labelright=True)
    ax2.grid(True)

    # accuracy plot
    ax2.plot(t, acc, color="red", label="Accuracy")
    ax2.plot(0, acc[0], color="red",marker = 'x', markersize = 15)
    ax2.plot(idx_mid, acc[idx_mid], color="red",marker = 'x', markersize = 15)
    ax2.plot(idx_max, acc[idx_max], color="red",marker = 'x', markersize = 15)
    ax2.tick_params(axis='y', labelcolor='red')

    # loss text
    ymin, ymax = ax1.get_ylim()
    d = ymax - ymin
    h = 0.06
    y = (loss[0] - ymin) / d + h
    ax2.text(0.01, 0.997, f'{(loss[0]):.5f}', color='blue', \
             va='top', ha='left', transform=ax1.transAxes, fontsize=15)
    y = (loss[idx_mid] - ymin) / d + h
    ax2.text(0.5, y, f'{(loss[idx_mid]):.5f}', color='blue', \
             va='center', ha='center', transform=ax1.transAxes, fontsize=15)
    y = (loss[-1] - ymin) / d + h
    ax2.text(0.99, y, f'{(loss[-1]):.5f}', color='blue', \
             va='center', ha='right', transform=ax1.transAxes, fontsize=15)

    # time text
    ax2.text(0.01, 0.015, f'{(times[0])/60:.5f}min', \
             va='center', ha='left', transform=ax1.transAxes)
    ax2.text(0.5, 0.015, f'{(times[idx_mid])/60:.5f}min', \
             va='center', ha='center', transform=ax1.transAxes)
    ax2.text(0.99, 0.015, f'{(times[-1])/60:.5f}min', \
             va='center', ha='right', transform=ax1.transAxes)

    # accuracy text
    y = acc[0] - h * np.sign(acc[0] - 0.3)
    ax2.text(0.01, y, f'{(acc[0]):.5f}', color='red', \
             va='center', ha='left', transform=ax1.transAxes, fontsize=15)
    y = acc[idx_mid] - h * np.sign(acc[idx_mid] - 0.3)
    ax2.text(0.5, y, f'{(acc[idx_mid]):.5f}', color='red', \
             va='center', ha='center', transform=ax1.transAxes, fontsize=15)
    y = acc[-1] - h * np.sign(acc[idx_mid] - 0.3)
    ax2.text(0.99, y, f'{(acc[-1]):.5f}', color='red', \
             va='center', ha='right', transform=ax1.transAxes, fontsize=15)


    # 凡例
    lines1, labels1 = ax1.get_legend_handles_labels()
    lines2, labels2 = ax2.get_legend_handles_labels()
    y = acc[-1] - 0.1 if acc[-1] > 0.8 else 0.99
    ax2.legend(lines2 + lines1, labels2 + labels1, \
               loc='upper right', bbox_to_anchor=(1, y))

    fig.tight_layout()
    plt.show()
    return plt

# 学習準備
model = Net().to(device)
criterion = nn.CrossEntropyLoss() # 損失関数＝クロスエントロピー
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, \
            weight_decay=1e-4)  # 最適化手法＝SGD、L2の係数＝1e-4
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, \
                            step_size=5, gamma=0.95) # 学習率更新

def print_metrics(epoch, X_true, Y_true):
    # 損失値、正解率を計算
    model.eval() # 検証モードに切り替え
    with torch.no_grad(): # 微分計算しない
        outputs = model(X_true) # 予測値（確率）を出力
        loss = criterion(outputs, Y_true) # 損失値計算
        pred = torch.argmax(outputs, dim=1) # 最大確率の添字取得
        acc = (pred == Y_true).float().mean() # 正解率計算
    now = time.perf_counter()
    # グラフ表示用に値を保存
    loss_list.append(loss.to("cpu"))
    acc_list.append(acc.to("cpu"))
    time_list.append(now - start)
    # 現在値表示
    print(f'Epoch: {epoch+1:3d}, Loss: {loss:.8f}, Accuracy: {acc:.5f}, ' \
          f'Time: {(now-start)/60:.5f}[min]')
    return

# 計測値を準備
loss_list, acc_list, time_list = [], [], []
start = time.perf_counter() #計測開始
print_metrics(-1, X_test, Y_test) # 学習前の状況表示

# 学習ループ
for epoch in range(100):
    model.train() # トレーニングモードに切り替え
    for xb, yb in train_loader: # ミニバッチ学習を実行
        optimizer.zero_grad() # 微分値を0にリセット
        xb, yb = xb.to(device), yb.to(device) # GPU対応
        outputs = model(xb) # 予測値（logits）を出力
        loss = criterion(outputs, yb) # 損失関数
        loss.backward() # 誤差逆伝播法
        optimizer.step() # 勾配降下法（パラメータ更新）

    print_metrics(epoch, X_test, Y_test) # 検証
    scheduler.step()  # 学習率更新

graph(loss_list, acc_list, time_list, title="PyTorch(GPU)")


# スケーリング（標準化）
#scaler = StandardScaler()
#X_train = scaler.fit_transform(X_train)
#X_test = scaler.transform(X_test)

    #with torch.no_grad():
#       _, predicted = torch.max(outputs, 1)
#        correct = (predicted == Y_test).sum().item()
#    correct = (pred == Y_test).float().mean()
#    total = Y_test.size(0)
#    acc = correct / total