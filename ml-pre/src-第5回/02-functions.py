# -*- coding: utf-8 -*-
"""資料作成#5-2-スライド用02-べた関数.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PRgMTOh3O4eQzRksqqVk-98qtY2rfhq7
"""

"""
02-00-activate.py
"""
# step function（階段関数）
def step_function(x):
    y = x > 0
    return y.astype(int)

# sotmax - 複数データセットに対応
def softmax(x):
    x = x if x.ndim >= 2 else x.reshape(1, x.size)
    M = np.max(x, axis=1, keepdims=True) # M: 各行ごとの最大値
    exp_x = np.exp(x - M)
    S = np.sum(exp_x, axis=1, keepdims=True) # 各行ごとの合計
    return exp_x / S

"""
02-01-digits.py
"""
import matplotlib.pyplot as plt
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split

""" digitsデータ読み込み """
digits = load_digits()
X_orig, Y_orig = digits.data, digits.target
X_train, X_test, Y_train, Y_test = train_test_split(
    X_orig, Y_orig, test_size=0.2, random_state=19)

def print_data():
    # データサイズ確認
    print(f'All: X:{X_orig.shape}, Y:{Y_orig.shape}, type:{type(X_orig)}, {type(Y_orig)}')
    print(f'Train: X_train:{X_train.shape}, Y_train:{Y_train.shape}')
    print(f'Test: X_test:{X_test.shape}, Y_test:{Y_test.shape}')

    # 念のため画像確認
    print(X_orig[0].shape)
    print(Y_orig[0])
    plt.figure(figsize=(2, 2))
    plt.imshow(X_orig[0].reshape(8, 8), cmap='gray')
    plt.show()

print_data()

"""
02-02-forward.py
"""
import numpy as np

""" 初期設定 """
np.random.seed(19) # 乱数初期化
input_size = 8 * 8 # 入力画像のサイズ
hidden_size = 64 # 隠し層、サイズは適当
output_size = 10 # 分類したい個数

""" 重みの初期化 """
p={}
p['W1'] = np.random.randn(input_size, hidden_size)
p['b1'] = np.zeros([1, hidden_size])
p['W2'] = np.random.randn(hidden_size, output_size)
p['b2'] = np.zeros([1, output_size])

def forward(x):
    """ 順伝播 """
    z1 = x @ p['W1'] + p['b1'] # x:入力, z1:ノード内出力
    a1 = step_function(z1) # a1:ノード外への出力
    z2 = a1 @ p['W2'] + p['b2'] # a1:入力, z2:ノード内出力
    a2 = softmax(z2) # a2:ノード外への出力
    return a2 # a2が最終出力

def activate_drop(X):
    return step_function(X)

"""
02-03-test.py
forwardの動作確認（正解不正解は不問）
"""
i = 0 # 何番目のテストデータかを指定
Y_pred = forward(X_test[i]) # i番目の画像を判別
print(Y_pred.shape) # リストサイズ確認→列が10ならOK
print(Y_pred) # 具体的な中身を表示→数字ならOK

k = np.argmax(Y_pred) # 一番確率の高い数字を取得
print(k, Y_test[i]) # 正解と並べて表示→8 7

"""
02-04-accuracy.py
accuracy＝正解率（100点満点のテストでの点数）
"""
def accuracy(Y_pred, Y):
    """ 正解率 """
    cnt = 0 # 正解数
    for i in range(len(Y_pred)):
        k = np.argmax(Y_pred[i]) # 最も確率が高いindexを取得
        if Y[i] == k: # i番目の正解Y[i]と予測値kを比較
            cnt += 1 # 正解ならカウント+1

    return cnt / len(Y)

Y_pred = forward(X_train) # X_trainから予測値取得
print(accuracy(Y_pred, Y_train))

"""
02-05-MSE.py
"""
def one_hot_vec(k):
    """ k番目の成分だけ1のワンホットベクトル """
    v = np.zeros(10, dtype=int)
    v[k] = 1
    return v

def one_hot_batch(k_list):
    """ n個のリストから(n,10)サイズのワンホットベクトルリストを作る """
    return np.array([one_hot_vec(k) for k in k_list])

def loss_mse(Y_pred, Y):
    """ 平均二乗誤差 """
    Y_vec = one_hot_batch(Y) # 正解をワンホットベクトル化
    rss = np.sum((Y_pred - Y_vec)**2, axis=1) # 列方向に二乗和
    return np.mean(rss)

def loss_func(X, Y):
    """ 損失関数（入力値をforwardして個別の関数に渡す） """
    Y_pred = forward(X) # データセット毎に予測値（確率）取得
    return loss_mse(Y_pred, Y)

print(loss_func(X_train, Y_train)) # 1.6888381853447831

"""
02-06-grad.py
べたコードで動かしてみる
"""
W = 'W1' # 微分対象のパラメータを絞る（今回はW1のどれか）
i, j = 2, 2 # W1の(2,2)成分に着目する（2,2にしたのは適当）
W_ij = p[W][i][j] # ij成分を保存（この後の計算のため）
h = 1e-1 # 微小値hを設定（ちょい粗めの設定）, 1e-1, 1e-5

# L(W+h)の計算
p[W][i][j] = W_ij + h # 先に微分対象のパラメータを+hしとく
L1 = loss_func(X_train, Y_train) # その状態でLを計算

# L(W-h)の計算
p[W][i][j] = W_ij - h # 先に微分対象のパラメータを-hしとく
L2 = loss_func(X_train, Y_train) # その状態でLを計算

# ij成分での微分値を算出
grad = (L1 - L2) / (2 * h)

# ij成分を元に戻す（今回は微分値の算出のみ、更新はしない）
p[W][i][j] = W_ij

print(grad) # 0.004309523370654711

"""
02-07-grad_numeric.py
関数化
"""
def grad_numeric(L, W):
    """ 微分（L:微分対象の関数、W:微分変数） """
    h = 1e-5 # 1e-1, 1e-5
    grad = np.zeros_like(p[W]) # 微分結果の格納先
    for i in range(p[W].shape[0]):
        for j in range(p[W].shape[1]):
            # 重みのij成分を保存しておく
            W_ij = p[W][i][j]

            # f(W+h)の計算
            p[W][i][j] = W_ij + h
            L1 = L()

            # f(W-h)の計算
            p[W][i][j] = W_ij - h
            L2 = L()

            # ij成分での微分値を算出
            grad[i][j] = (L1 - L2) / (2 * h)

            # 重みij成分を元に戻す
            p[W][i][j] = W_ij

    return grad

# 動作確認
#L = lambda: loss_func(X_train, Y_train) # 微分対象
#grads = {}
#grads['W1'] = grad_numeric(L, 'W1')
#print(grads['W1'].shape, np.sum(grads['W1']))

"""
02-08-grad_weight_all.py
全重みパラメータに対して微分を実行
"""
def grad_weight_all(X, Y):
    L = lambda: loss_func(X, Y) # 微分対象
    grads = {}
    grads['W1'] = grad_numeric(L, 'W1')
    grads['b1'] = grad_numeric(L, 'b1')
    grads['W2'] = grad_numeric(L, 'W2')
    grads['b2'] = grad_numeric(L, 'b2')
    return grads

# 動作確認
#grads = grad_weight_all(X_train, Y_train)
#print(grads['W2'].shape, np.sum(grads['W2']))

"""
02-09-train.py
パラメータ更新を200回
"""
def train(X, Y):
    print_metrics(X, Y) # 初期状態表示

    for i in range(max_epoch):
        print(f'loop: {i+1} -> ', end='')

        # 微分値を取得
        grads = grad_weight_all(X, Y)

        # パラメータ更新
        for key in grads.keys():
            p[key] = p[key] - lr * grads[key]

        print_metrics(X_test, Y_test) # エポック毎に検証

    return

"""
02-10-print_metrics.py
損失関数、正解率、経過時間をprint
"""
def print_metrics(x, y):
    # 現在値を取得
    loss = loss_func(x, y)
    acc = accuracy(forward(x), y)
    now = time.perf_counter()
    # グラフ表示用に値を保存
    loss_list.append(loss)
    acc_list.append(acc)
    time_list.append(now - start)
    # 現在値表示
    print(f'Loss: {loss:.10f}, Accuracy: {acc:.5f}, ' \
          f'Time: {(now-start)/60:.5f}[min]')
    return loss, acc, now

"""
02-11-graph.py
グラフ表示
"""
def graph(loss, acc, times, y_range=[0,2], y_step=0.2, title="loss, accuracy"):
    plt.rcParams["font.size"] = 14
    fig, ax1 = plt.subplots(figsize=(8.0, 6.0))
    step = len(loss)
    idx_mid, idx_max = int(step/2), step - 1
    t = np.linspace(0, idx_max, step)  # start, end, step

    # loss軸設定
    ax1.set_title(title, fontsize=13)
    ax1.set_xlabel('epoch')
    ax1.set_xlim([-5, idx_max+5]) # -1
    ax1.tick_params(axis='y', labelcolor='blue')
    ax1.grid(True, axis='x')

    # loss plot
    ax1.plot(t, loss, color="blue", label="Loss")
    ax1.plot(0, loss[0], color="blue",marker = 'x', markersize = 15)
    ax1.plot(idx_mid, loss[idx_mid], color="blue",marker = 'x', markersize = 15)
    ax1.plot(idx_max, loss[idx_max], color="blue",marker = 'x', markersize = 15)

    # accuracy軸設定
    ax2 = ax1.twinx()
    ax2.set_ylabel('accuracy', color='red')
    ax2.set_ylim([0, 1])
    ax2.set_yticks(np.arange(0, 1, 0.1))
    ax2.tick_params(axis='y', right=True, labelright=True)
    ax2.grid(True)

    # accuracy plot
    ax2.plot(t, acc, color="red", label="Accuracy")
    ax2.plot(0, acc[0], color="red",marker = 'x', markersize = 15)
    ax2.plot(idx_mid, acc[idx_mid], color="red",marker = 'x', markersize = 15)
    ax2.plot(idx_max, acc[idx_max], color="red",marker = 'x', markersize = 15)
    ax2.tick_params(axis='y', labelcolor='red')

    # loss text
    ymin, ymax = ax1.get_ylim()
    d = ymax - ymin
    h = 0.06
    y = (loss[0] - ymin) / d + h
    ax2.text(0.01, 0.997, f'{(loss[0]):.5f}', color='blue', \
             va='top', ha='left', transform=ax1.transAxes, fontsize=15)
    y = (loss[idx_mid] - ymin) / d + h
    ax2.text(0.5, y, f'{(loss[idx_mid]):.5f}', color='blue', \
             va='center', ha='center', transform=ax1.transAxes, fontsize=15)
    y = (loss[-1] - ymin) / d + h
    ax2.text(0.99, y, f'{(loss[-1]):.5f}', color='blue', \
             va='center', ha='right', transform=ax1.transAxes, fontsize=15)

    # time text
    ax2.text(0.01, 0.015, f'{(times[0])/60:.5f}min', \
             va='center', ha='left', transform=ax1.transAxes)
    ax2.text(0.5, 0.015, f'{(times[idx_mid])/60:.5f}min', \
             va='center', ha='center', transform=ax1.transAxes)
    ax2.text(0.99, 0.015, f'{(times[-1])/60:.5f}min', \
             va='center', ha='right', transform=ax1.transAxes)

    # accuracy text
    y = acc[0] - h * np.sign(acc[0] - 0.3)
    ax2.text(0.01, y, f'{(acc[0]):.5f}', color='red', \
             va='center', ha='left', transform=ax1.transAxes, fontsize=15)
    y = acc[idx_mid] - h * np.sign(acc[idx_mid] - 0.3)
    ax2.text(0.5, y, f'{(acc[idx_mid]):.5f}', color='red', \
             va='center', ha='center', transform=ax1.transAxes, fontsize=15)
    y = acc[-1] - h * np.sign(acc[idx_mid] - 0.3)
    ax2.text(0.99, y, f'{(acc[-1]):.5f}', color='red', \
             va='center', ha='right', transform=ax1.transAxes, fontsize=15)

    # 凡例
    lines1, labels1 = ax1.get_legend_handles_labels()
    lines2, labels2 = ax2.get_legend_handles_labels()
    y = acc[-1] - 0.1 if acc[-1] > 0.5 else 0.9
    ax2.legend(lines2 + lines1, labels2 + labels1, \
               loc='upper right', bbox_to_anchor=(1, y))

    fig.tight_layout()
    plt.show()
    return plt

"""
02-12-exec.py
学習実行
"""
import time

max_epoch = 400
lr = 1e-1
loss_list, acc_list, time_list = [], [], []

start = time.perf_counter() # 計測開始
train(X_train, Y_train)
graph(loss_list, acc_list, time_list, title='Retro Neural Network')

"""
02-13-train-minibatch.py
学習ループをミニバッチに変更
"""
def train_minibatch(X, Y):
    print_metrics(X_test, Y_test) # 初期状態表示

    # エポック毎にループ
    for i in range(max_epoch):
        print(f'epoch: {i+1} -> ', end='')
        # 1エポック内で訓練データをシャッフル
        num = X.shape[0] # データ個数取得
        indices = np.arange(num) # データ個数分の順列を取得
        np.random.shuffle(indices) # シャッフル

        # バッチサイズごとにループ
        for j in range(0, num, batch_size):
            # バッチセットを取得
            batch_indices = indices[j:min(j + batch_size, num)]
            X_batch = X[batch_indices]
            Y_batch = Y[batch_indices]
            # パラメータ更新
            grads = grad_weight_all(X_batch, Y_batch)
            # パラメータ更新
            for key in grads.keys():
                p[key] = p[key] - lr * grads[key]

        print_metrics(X_test, Y_test) # エポック毎に検証

    return loss_list, acc_list

"""
02-14-exec-minibatch.py
ミニバッチ学習実行
（02-12-exec.pyをパスして実行）
"""
import time

max_epoch = 400
lr = 1e-1
batch_size = 50
loss_list, acc_list, time_list = [], [], []

start = time.perf_counter() #計測開始
train_minibatch(X_train, Y_train)
graph(loss_list, acc_list, time_list, title='minibatch')

"""
02-15-exec-online.py
オンライン学習実行
"""
import time

max_epoch = 50
lr = 1e-1
batch_size = 1 # 変更はここだけ
loss_list, acc_list, time_list = [], [], []

start = time.perf_counter() #計測開始
train_minibatch(X_train, Y_train)
graph(loss_list, acc_list, time_list, title='batchsize = 1')

"""
02-16-refactor.py
リファクタリング（関数内部を変更）
"""
def accuracy(Y_pred, Y):
    """ 正解率（Python的実装） """
    return np.mean(np.argmax(Y_pred, axis=1) == Y)

def one_hot_batch(k_list, num_classes=10):
    """ n個のリストから(n,10)サイズのワンホットベクトルリストを作る """
    return np.eye(num_classes)[k_list]

def grad_numeric(L, W):
    """ 微分（L:微分対象の関数、W:微分変数） """
    h = 1e-5
    grad = np.zeros_like(p[W]) # 微分結果の格納先
    iter = np.nditer(p[W], flags=['multi_index'])
    while not iter.finished:
        idx = iter.multi_index
        original = p[W][idx].copy()

        p[W][idx] = original + h
        L1 = L()
        p[W][idx] = original - h
        L2 = L()
        grad[idx] = (L1 - L2) / (2 * h)

        p[W][idx] = original
        iter.iternext()
    return grad

"""
02-17-exec-refactor.py
リファクタ版を実行（中身は02-14と同じ）
"""
import time

max_epoch = 400
lr = 1e-1
batch_size = 50
loss_list, acc_list, time_list = [], [], []

start = time.perf_counter() #計測開始
train_minibatch(X_train, Y_train)
graph(loss_list, acc_list, time_list, title='refactor')